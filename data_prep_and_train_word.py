# data_prep_and_train_word.py - å˜èªãƒ»æ•°å€¤ãƒ™ãƒ¼ã‚¹ã®OCRãƒ¢ãƒ‡ãƒ«è¨“ç·´
import cv2
import json
import pathlib
import random
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

SCREEN_W, SCREEN_H = 1920, 1080
TARGET_W = 128  # å¹…
TARGET_H = 32   # é«˜ã•
OUT_DIR = pathlib.Path("dataset_words")

# å˜èªãƒ»æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¯ãƒ©ã‚¹ãƒªã‚¹ãƒˆï¼ˆå®Ÿéš›ã®ç”»åƒã‹ã‚‰å–å¾—ã—ãŸæ–‡å­—åˆ—ï¼‰
CLASSES = [
    # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åï¼ˆä¾‹ï¼‰
    "ãƒ“ãƒƒã‚°ãƒ™ã‚¢", "å°ã•ãªå­ç¾Š", "ã‚¢ãƒ³ã‚³ãƒ¼ãƒ«", "ä»Šæ±", "ã‚«ã‚«ãƒ­",
    # COSTé–¢é€£
    "COST", "1", "2", "3", "4",
    # ãƒ¬ãƒ™ãƒ«
    "+0", "+5", "+10", "+15", "+20", "+25",
    # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å
    "å‡ç¸®ãƒ€ãƒ¡ãƒ¼ã‚¸ã‚¢ãƒƒãƒ—", "æ”»æ’ƒåŠ›", "é˜²å¾¡åŠ›", "HP", "ä¼šå¿ƒç‡", "ä¼šå¿ƒãƒ€ãƒ¡ãƒ¼ã‚¸",
    "ã‚¨ãƒãƒ«ã‚®ãƒ¼å›å¾©åŠ¹ç‡", "å±æ€§ãƒ€ãƒ¡ãƒ¼ã‚¸ã‚¢ãƒƒãƒ—",
    # ã‚µãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å
    "æ”»æ’ƒåŠ›", "æ”»æ’ƒåŠ›%", "é˜²å¾¡åŠ›", "é˜²å¾¡åŠ›%", "HP", "HP%",
    "ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«", "ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ€ãƒ¡ãƒ¼ã‚¸", "ä¼šå¿ƒç‡", "ä¼šå¿ƒãƒ€ãƒ¡ãƒ¼ã‚¸",
    "å…±é³´è§£æ”¾ãƒ€ãƒ¡ãƒ¼ã‚¸ã‚¢ãƒƒãƒ—", "å…±é³´ã‚¹ã‚­ãƒ«ãƒ€ãƒ¡ãƒ¼ã‚¸ã‚¢ãƒƒãƒ—", "é€šå¸¸æ”»æ’ƒãƒ€ãƒ¡ãƒ¼ã‚¸ã‚¢ãƒƒãƒ—",
    "é‡æ’ƒãƒ€ãƒ¡ãƒ¼ã‚¸ã‚¢ãƒƒãƒ—", "ã‚¨ãƒãƒ«ã‚®ãƒ¼å›å¾©åŠ¹ç‡",
    # æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
    "30.0%", "100", "9.4%", "15.0%", "9.9%", "8.6%", "7.1%",
    "50", "75", "150", "200", "10%", "20%", "25%",
    "12.0%", "18.0%", "6.0%", "4.5%",
    # ãã®ä»–
    "", "---"  # ç©ºç™½ãƒ»èªè­˜å¤±æ•—ç”¨
]
NUM_CLASSES = len(CLASSES)
CLASS_TO_ID = {c: i for i, c in enumerate(CLASSES)}

# å˜èªãƒ»æ•°å€¤ãƒ™ãƒ¼ã‚¹ã®åº§æ¨™å®šç¾©ï¼ˆæ·»ä»˜ç”»åƒã®å³å´ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼‰
# å®Ÿéš›ã®åº§æ¨™ã¯ annotator.html ã§æ¸¬å®šã—ã¦ãã ã•ã„
WORD_BOXES = [
    # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚¨ãƒªã‚¢ï¼ˆä¸Šéƒ¨ï¼‰
    {"label": "ã‚­ãƒ£ãƒ©å", "type": "text", "x": 1015, "y": 105, "w": 110, "h": 30},
    {"label": "COSTè¡¨è¨˜", "type": "text", "x": 1015, "y": 128, "w": 80, "h": 22},
    {"label": "COSTå€¤", "type": "number", "x": 1100, "y": 128, "w": 30, "h": 22},
    {"label": "ãƒ¬ãƒ™ãƒ«", "type": "number", "x": 1015, "y": 150, "w": 40, "h": 22},

    # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    {"label": "ãƒ¡ã‚¤ãƒ³å", "type": "text", "x": 1015, "y": 172, "w": 200, "h": 28},
    {"label": "ãƒ¡ã‚¤ãƒ³å€¤", "type": "number", "x": 1240, "y": 172, "w": 80, "h": 28},

    # ã‚µãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹1
    {"label": "ã‚µãƒ–1å", "type": "text", "x": 1015, "y": 205, "w": 120, "h": 26},
    {"label": "ã‚µãƒ–1å€¤", "type": "number", "x": 1255, "y": 205, "w": 65, "h": 26},

    # ã‚µãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹2
    {"label": "ã‚µãƒ–2å", "type": "text", "x": 1015, "y": 230, "w": 100, "h": 26},
    {"label": "ã‚µãƒ–2å€¤", "type": "number", "x": 1255, "y": 230, "w": 65, "h": 26},

    # ã‚µãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹3
    {"label": "ã‚µãƒ–3å", "type": "text", "x": 1015, "y": 255, "w": 170, "h": 26},
    {"label": "ã‚µãƒ–3å€¤", "type": "number", "x": 1240, "y": 255, "w": 80, "h": 26},

    # ã‚µãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹4
    {"label": "ã‚µãƒ–4å", "type": "text", "x": 1015, "y": 280, "w": 130, "h": 26},
    {"label": "ã‚µãƒ–4å€¤", "type": "number", "x": 1255, "y": 280, "w": 65, "h": 26},

    # ã‚µãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹5
    {"label": "ã‚µãƒ–5å", "type": "text", "x": 1015, "y": 302, "w": 200, "h": 26},
    {"label": "ã‚µãƒ–5å€¤", "type": "number", "x": 1240, "y": 302, "w": 80, "h": 26},

    # ã‚µãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹6ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
    {"label": "ã‚µãƒ–6å", "type": "text", "x": 1015, "y": 325, "w": 200, "h": 26},
    {"label": "ã‚µãƒ–6å€¤", "type": "number", "x": 1240, "y": 325, "w": 80, "h": 26},
]

def extract_words(img_path):
  """ç”»åƒã‹ã‚‰å˜èªãƒ»æ•°å€¤ãƒ‘ãƒƒãƒã‚’æŠ½å‡º"""
  img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
  if img is None:
    return []

  h, w = img.shape
  sx, sy = w / SCREEN_W, h / SCREEN_H
  patches = []

  for box in WORD_BOXES:
    x, y, bw, bh = box["x"], box["y"], box["w"], box["h"]
    xs, ys = int(x * sx), int(y * sy)
    ws, hs = int(bw * sx), int(bh * sy)

    crop = img[ys:ys + hs, xs:xs + ws]

    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ä¿æŒã—ã¦ãƒªã‚µã‚¤ã‚º
    target_h = TARGET_H
    target_w = int(ws * target_h / hs) if hs > 0 else TARGET_W
    target_w = min(target_w, TARGET_W)

    resized = cv2.resize(crop, (target_w, target_h),
                         interpolation=cv2.INTER_AREA)

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦å›ºå®šã‚µã‚¤ã‚ºã«
    padded = np.zeros((target_h, TARGET_W), dtype=np.uint8)
    padded[:, :target_w] = resized

    patches.append((box["label"], padded))

  return patches

def augment(patch):
  """ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ"""
  # ãƒ©ãƒ³ãƒ€ãƒ ãªè¼åº¦èª¿æ•´
  scale = random.uniform(0.8, 1.2)
  patch = np.clip(patch * scale, 0, 255).astype(np.uint8)

  # ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚º
  if random.random() < 0.3:
    noise = np.random.normal(0, 3, patch.shape)
    patch = np.clip(patch + noise, 0, 255).astype(np.uint8)

  return patch

def load_dataset(img_paths, aug_per_image=5):
  """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰"""
  X, Y = [], []

  for img_path in img_paths:
    patches = extract_words(str(img_path))

    for label, patch in patches:
      # ã‚ªãƒªã‚¸ãƒŠãƒ«
      X.append(patch.astype(np.float32) / 255.0)
      Y.append(CLASS_TO_ID.get(label, CLASS_TO_ID[""]))  # æœªçŸ¥ã®å ´åˆã¯ç©ºç™½

      # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿
      for _ in range(aug_per_image):
        aug_patch = augment(patch)
        X.append(aug_patch.astype(np.float32) / 255.0)
        Y.append(CLASS_TO_ID.get(label, CLASS_TO_ID[""]))

  X = np.array(X).reshape(-1, TARGET_H, TARGET_W, 1)
  Y = tf.keras.utils.to_categorical(Y, NUM_CLASSES)

  return X, Y

def build_model():
  """å˜èªèªè­˜ç”¨ã®CNNãƒ¢ãƒ‡ãƒ«ï¼ˆã‚ˆã‚Šå¤§ããªå…¥åŠ›ç”¨ï¼‰"""
  model = keras.Sequential([
      layers.Input(shape=(TARGET_H, TARGET_W, 1)),

      # ç•³ã¿è¾¼ã¿å±¤ï¼ˆã‚ˆã‚Šæ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰
      layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
      layers.MaxPooling2D((2, 2)),
      layers.Dropout(0.25),

      layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
      layers.MaxPooling2D((2, 2)),
      layers.Dropout(0.25),

      layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
      layers.MaxPooling2D((2, 2)),
      layers.Dropout(0.25),

      # å…¨çµåˆå±¤
      layers.Flatten(),
      layers.Dense(256, activation='relu'),
      layers.Dropout(0.5),
      layers.Dense(128, activation='relu'),
      layers.Dropout(0.3),
      layers.Dense(NUM_CLASSES, activation='softmax')
  ])

  model.compile(
      optimizer='adam',
      loss='categorical_crossentropy',
      metrics=['accuracy']
  )

  return model

def main():
  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”»åƒã‚’æ¤œç´¢
  img_paths = list(pathlib.Path(".").glob("*.png")) + \
      list(pathlib.Path(".").glob("*.jpg"))

  if not img_paths:
    print("âŒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    print("   ã‚²ãƒ¼ãƒ ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç”»é¢ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆ1920x1080æ¨å¥¨ï¼‰ã‚’")
    print("   ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
    return

  print(f"ğŸ“¸ ç”»åƒæ•°: {len(img_paths)}")
  print(f"ğŸ“¦ ã‚¯ãƒ©ã‚¹æ•°: {NUM_CLASSES}")
  print(f"ğŸ“ å…¥åŠ›ã‚µã‚¤ã‚º: {TARGET_H}x{TARGET_W}")

  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
  print("\nğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ä¸­...")
  X, Y = load_dataset(img_paths, aug_per_image=10)
  print(f"âœ“ ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}")

  # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
  print("\nğŸ—ï¸ ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ä¸­...")
  model = build_model()
  model.summary()

  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
  print("\nğŸš€ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
  history = model.fit(
      X, Y,
      batch_size=32,
      epochs=30,
      validation_split=0.15,
      verbose=1
  )

  # ä¿å­˜
  model.save("ocr_model_word.keras")
  print("\nâœ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: ocr_model_word.keras")

  # ã‚¯ãƒ©ã‚¹ãƒªã‚¹ãƒˆä¿å­˜
  OUT_DIR.mkdir(exist_ok=True)
  with open(OUT_DIR / "classes.json", "w", encoding="utf-8") as f:
    json.dump(CLASSES, f, ensure_ascii=False, indent=2)
  print(f"âœ“ ã‚¯ãƒ©ã‚¹ãƒªã‚¹ãƒˆä¿å­˜: {OUT_DIR}/classes.json")

  # æœ€çµ‚ç²¾åº¦
  final_acc = history.history['accuracy'][-1]
  final_val_acc = history.history.get('val_accuracy', [0])[-1]
  print(f"\nğŸ“Š æœ€çµ‚ç²¾åº¦: {final_acc*100:.2f}% (æ¤œè¨¼: {final_val_acc*100:.2f}%)")

if __name__ == "__main__":
  main()
